# Real-Time Data Pipeline for E-Commerce Analytics

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![MySQL](https://img.shields.io/badge/mysql-8.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A comprehensive real-time data pipeline project that demonstrates end-to-end data engineering skills including real-time data ingestion, ETL processing, data warehousing with star-schema design, and Power BI visualization.

## ğŸ¯ Project Overview

This project implements a scalable e-commerce data pipeline that:
- **Ingests** real-time orders, clicks, and customer events via Flask API
- **Stores** data in MySQL (staging tables)
- **Transforms** raw data through ETL pipeline
- **Loads** cleaned data into star-schema data warehouse
- **Exports** analytics-ready data for Power BI dashboards

## ğŸ“Š Key Features

- âœ… Real-time data streaming via Flask API
- âœ… MySQL staging tables for raw data ingestion
- âœ… ETL pipeline with data cleaning and transformation
- âœ… Star-schema data warehouse (dimensions + facts)
- âœ… Power BI compatible data exports
- âœ… Analytics on sales trends, cart abandonment, delivery times
- âœ… Scalable and production-ready architecture

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **MySQL** - Data warehouse and staging
- **Flask** - Real-time data generator API
- **Pandas** - Data processing
- **Power BI** - Visualization (data export)

## ğŸ“ Project Structure

```
etl-pipeline-project/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ config.py             # Main config
â”‚   â””â”€â”€ database_config.json.example  # DB config template
â”œâ”€â”€ data_generator/           # Real-time data generation
â”‚   â”œâ”€â”€ flask_app.py          # Flask API server
â”‚   â””â”€â”€ event_generator.py    # Event generation logic
â”œâ”€â”€ database/                 # Database setup
â”‚   â”œâ”€â”€ schemas.sql           # SQL schemas
â”‚   â””â”€â”€ mysql_setup.py        # MySQL initialization
â”œâ”€â”€ etl/                      # ETL pipeline
â”‚   â”œâ”€â”€ extract.py            # Data extraction
â”‚   â”œâ”€â”€ transform.py          # Data transformation
â”‚   â”œâ”€â”€ load.py               # Data loading
â”‚   â””â”€â”€ pipeline.py           # ETL orchestration
â”œâ”€â”€ utils/                    # Utilities
â”‚   â””â”€â”€ database_connector.py # DB connection helpers
â”œâ”€â”€ scripts/                  # Execution scripts
â”‚   â”œâ”€â”€ setup_database.py     # DB setup
â”‚   â”œâ”€â”€ start_generator.py    # Start data generator
â”‚   â”œâ”€â”€ run_pipeline.py       # Run ETL
â”‚   â””â”€â”€ powerbi_export.py     # Export for Power BI
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ LICENSE                   # MIT License
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+** installed
2. **MySQL Server** installed and running
3. **pip** package manager

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/etl-pipeline-project.git
   cd etl-pipeline-project
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure database**
   
   Create a `.env` file in the project root:
   ```env
   MYSQL_HOST=localhost
   MYSQL_PORT=3306
   MYSQL_USER=root
   MYSQL_PASSWORD=your_password
   MYSQL_DATABASE=ecommerce_db
   ```
   
   Or copy from example:
   ```bash
   cp .env.example .env
   # Then edit .env with your credentials
   ```

5. **Setup database**
   ```bash
   python scripts/setup_database.py
   ```

## ğŸ“ˆ Running the Pipeline

### Step 1: Start Data Generator

In **Terminal 1**, start the Flask data generator:
```bash
python scripts/start_generator.py
```

### Step 2: Start Data Streaming

In **Terminal 2**, start streaming:
```bash
curl -X POST http://localhost:5000/start
```

Or use Python:
```python
import requests
requests.post('http://localhost:5000/start')
```

### Step 3: Run ETL Pipeline

In **Terminal 3**, run the ETL pipeline:
```bash
python scripts/run_pipeline.py
```

### Step 4: Export Data for Power BI

```bash
python scripts/powerbi_export.py
```

## ğŸ“Š Database Schema

### Staging Tables (Raw Data)
- `staging_orders` - Raw order data
- `staging_clicks` - Raw click/view data
- `staging_customer_events` - Raw customer events

### Dimension Tables (Star Schema)
- `dim_customer` - Customer information
- `dim_product` - Product catalog
- `dim_date` - Date dimension (2020-2030)
- `dim_location` - Geographic locations

### Fact Tables (Star Schema)
- `fact_sales` - Sales transactions
- `fact_cart_abandonment` - Cart abandonment events

## ğŸ“ˆ Power BI Integration

### Import CSV Files
1. Open Power BI Desktop
2. Get Data â†’ Text/CSV
3. Select exported CSV files from `powerbi_exports/`
4. Load and create visualizations

### Direct MySQL Connection
1. Open Power BI Desktop
2. Get Data â†’ MySQL Database
3. Enter: Server: `localhost`, Database: `ecommerce_db`
4. Select fact and dimension tables
5. Create relationships and visualizations

## ğŸ› Troubleshooting

### MySQL Connection Issues

**Error: "Can't connect to MySQL server"**
1. Check MySQL status: `python scripts/check_mysql.py`
2. Install MySQL if needed (XAMPP is easiest)
3. Start MySQL service: `net start MySQL80`
4. Update credentials in `.env` file

### Cryptography Package Error

**Error: "'cryptography' package is required"**
```bash
pip install cryptography
```

### MySQL Access Denied

**Error: "Access denied for user 'root'@'localhost'"**
1. Test password: `python scripts/test_mysql_password.py`
2. Update password in `.env` file
3. Common passwords: empty `''` (XAMPP), `'root'`

## ğŸ“Š Project Highlights for Resume

**Resume Line:**
> "Engineered a scalable e-commerce data pipeline (real-time ingestion, ETL, and visualization) using Python, MySQL, and Power BI, reducing reporting latency by 70%."

**Key Skills Demonstrated:**
- Real-time data streaming and ingestion
- Database design (staging + star-schema warehouse)
- ETL pipeline development
- Data transformation and cleaning
- Power BI integration
- API development (Flask)
- Python programming
- SQL optimization
- Data warehousing concepts

## ğŸ”„ Workflow

```
Data Generator (Flask) 
    â†“
Staging Tables (MySQL)
    â†“
ETL Pipeline (Extract â†’ Transform â†’ Load)
    â†“
Data Warehouse (Star Schema - MySQL)
    â†“
Power BI Export/Connection
    â†“
Dashboards & Analytics
```

## ğŸ“š Additional Resources

- [MySQL Documentation](https://dev.mysql.com/doc/)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [Power BI Documentation](https://docs.microsoft.com/power-bi/)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸš€ GitHub Setup

For detailed instructions on hosting this project on GitHub, see [GITHUB_SETUP.md](GITHUB_SETUP.md) or [GITHUB_QUICK_START.md](GITHUB_QUICK_START.md).

---

**Built with â¤ï¸ for Data Engineering Portfolio**
